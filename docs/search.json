[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Data handling for biological sciences",
    "section": "",
    "text": "Welcome\nLorem ipsum dolor sit amet, consectetur adipiscing elit. Nam scelerisque mi velit, vel eleifend nulla egestas quis. Nulla mollis diam nec lacus aliquam aliquam. Etiam rhoncus est id velit tincidunt, a luctus eros viverra. Quisque malesuada in libero dignissim semper. Nullam euismod, dolor vel suscipit cursus, risus turpis posuere nunc, vitae lacinia odio tortor sed neque. Ut molestie sem urna, non ultricies tortor maximus at. Quisque eget leo non neque ultrices ullamcorper ac id risus. Aenean sit amet nisi tellus. Donec mattis est in felis ornare, quis sollicitudin magna imperdiet. Sed odio dolor, tristique nec rutrum sed, imperdiet eget nulla. Proin venenatis quam ut posuere placerat. Vestibulum faucibus tincidunt mauris, eget aliquam leo mattis ac. Vestibulum pulvinar euismod maximus.\nSuspendisse ut congue nulla. Donec dictum ex ipsum, volutpat scelerisque quam feugiat vel. Ut suscipit faucibus nisi eu fringilla. Nunc lectus lorem, ultricies nec ullamcorper non, vestibulum id sem. Aliquam cursus, est eget blandit ultricies, felis arcu ornare est, eget aliquet felis eros vel sapien. Aliquam erat volutpat. Praesent suscipit diam iaculis, ultricies sapien vel, luctus augue. Donec eleifend elementum euismod. Nulla a nisi volutpat nibh volutpat ullamcorper quis id risus. Nam id orci maximus, faucibus purus et, placerat nisi. Donec non malesuada nisi, eu viverra turpis."
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "1  Introduction",
    "section": "",
    "text": "To illustrate the importance of studying data correctly to obtain reliable conclusions, let’s consider an example. In Italy, between 10/12/2021 and 09/01/2022, the rate of hospitalizations in intensive care due to COVID-19 for non-vaccinated individuals (age-standardized for the population aged ≥ 12 years) was 35.6 per 100000 inhabitants. In contrast, for vaccinated individuals with a full cycle of ≤ 120 days, the same index was 2.0 per 100000 inhabitants (data from the Istituto Superiore di Sanità, ISS). This result suggests that the rate of hospitalization in intensive care for unvaccinated individuals is about eighteen times higher than for vaccinated individuals.\nHowever, this is only a partial conclusion. To better understand the effect of vaccination on hospitalization, we need to know the ratio of vaccinated to unvaccinated individuals. This ratio is essential in interpreting the data since the distribution of people between the vaccinated and unvaccinated groups affects the conclusion. If the ratio is around one, the previous conclusion has a certain interpretation. But if the ratio is near to zero, where the number of vaccinated individuals is much greater than the number of unvaccinated individuals, the data have a different weight, and the effect of vaccination is much more significant.\nThis example demonstrates the importance of reading, manipulating, and drawing conclusions from data correctly.\nIn the field of biochemistry, a fundamental aspect of studying proteins is to understand their properties and functions. However, analyzing proteins involves handling and analyzing complex data, which requires a strong foundation in data handling and analysis.\nOne example of the importance of data analysis in protein research is the statistical analysis of the chemical and physical properties of proteins. By using statistical methods, patterns and trends can be identified in these properties, such as molecular weight, charge, and hydrophobicity. These analyses can help identify common characteristics among different types of proteins and provide insights into their functions and behaviors.\nAnother example is the comparative analysis of protein stability. Proteins can have different levels of thermal stability, and understanding these differences is critical to many areas of study. By comparing the thermal stability of different proteins, factors that contribute to stability can be identified and this information can be used to design more stable proteins for a range of applications.\nIn addition to these examples, data analysis is essential for studying protein-protein interactions, predicting protein structures, and many other areas of study in biochemistry. With the growing field of bioinformatics and computational biology, the ability to handle and analyze large data sets has become increasingly important in this field.\nIn conclusion, having skills in data handling and analysis is crucial for studying proteins in biochemistry. By understanding these skills, complex data can be analyzed to identify patterns and trends, make meaningful comparisons between different proteins, and gain insights into their functions and behaviors. Ultimately, these skills are essential for advancing our understanding of proteins and their role in biochemistry.\nThe course will cover the basic concepts of statistics, data analysis, and data visualization using specific examples of data relating to biological systems. Practical examples will be reported in the R programming language."
  },
  {
    "objectID": "desc_stats.html#difference-between-population-and-sample",
    "href": "desc_stats.html#difference-between-population-and-sample",
    "title": "2  The Basics of Descriptive Statistics",
    "section": "2.1 Difference between population and sample",
    "text": "2.1 Difference between population and sample\nA population refers to the entire group of items from which one wants to draw conclusions, while a sample is a specific subset of the entire population from which data is collected. Therefore, the sample size is always smaller than the total population size. In practical terms, the use of a sample is particularly important when the population is large and it is not feasible to analyze every component of it. For instance, in order to determine the voting percentages of each political party, theoretically, each adult would need to be asked about their favorite party. However, this is a complex and almost impossible task, which is why companies that propose polls for voting percentage work with samples, such as exit polls.\nThe critical issue is to investigate the conditions under which the prediction is close to the reality, as well as to define what “close” means in this context. Using statistical analysis, it is possible to use sample data to obtain estimates or test hypotheses about population data. Ideally, when the sample size is similar to the population size, the results can be estimated reliably, since the sample is necessarily a good representation of the population. However, in most cases, the population is only a theoretical concept, and it is not feasible to work with it. Therefore, it is necessary to choose a criterion to appropriately select the working sample in such a way that it is as representative as possible of the population. The number of objects/subjects that should be included in a sample depends on multiple factors, such as the size and variability of the population or the purpose of the analysis.\nThere are two types of approaches for sampling population data to obtain a suitable sample: (i) probability sampling methods and (ii) non-probability sampling methods.\nProbability sampling methods Probability sampling means that each element of the population can be selected. Typically this is the best method if you want to produce results that are representative of the whole population. There are four main types of probability sample, below all these will be briefly discuss.\n\nSimple random sampling each element of the population has the same probability of being selected for the definition of the sample. Therefore, the whole population can be subject to random extraction. A useful tool, in computational terms, is the random number generator (see next example in code R).\nSystematic sampling the basic idea is to imagine the population dataset as a list and select the elements of the sample with a specific regularity (considering fixed steps). Obviously, this method is very similar to the previous one if the dataset is uniform. On the contrary, if the population dataset has hidden patters, the sample risks being out of balance with the population.\nStratified sampling In this case, the data is stratified (subpopulation groups are created from the population) in such a way that each group is well represented in the population. At this point, for the construction of the sample, members can be randomly selected from each sample in order to maintain the proportion with the initial population.\nCluster sampling consists in dividing the population into a certain number of groups, which are obtained thanks to a clustering analysis (we will see this in the next chapters). Each subgroup should have similar characteristics to the entire sample, allowing us to randomly sample entire subgroups selections.\n\nNon-Probability sampling methods In a non-probability sample, members belonging to the sample are selected based on non-random criteria, meaning that not every individual has a chance of being included. This type of sample is easier and cheaper to access, but it has a higher risk of sampling bias. Also in this case there are four different kind of non probability sampling approaches.\n\nA convenience sample is simply made up of subjects/objects that are more accessible to the researcher. This is typically a first exploratory approach, which is simple and inexpensive, which can be useful for initial data collection. However, the most important limitation is that there is no way to state whether the sample is representative of the population and so, in principle, it cannot produce generalizable results.\nVoluntary response sampling is a method that refers to all those data that derive from voluntary sharing. One common bias is self-selection bias, where individuals who choose to participate in the study may be different from those who choose not to participate, leading to a non-representative sample. Additionally, voluntary response sampling may attract individuals with extreme opinions or experiences, which can also lead to biased results.\nPurposive sampling is a type of sampling, also known as judgment sampling, which involves the use of the researcher’s experience, given that the sample is selected in a specific way considering a series of constraints useful for the purposes of the research. Purposive sampling is used in statistics where the researcher deliberately chooses individuals or units to be included in the sample based on a specific purpose or criterion. In this method, the researcher selects participants who are believed to be most appropriate or relevant for the research question, rather than selecting them randomly.\nSnowball sampling is characteristic for samples made up of people. If the population is difficult to access, snowball sampling can be used to engage participants via other participants (via a first neighbors selection criterion).\n\n\n# In the following R code, practical application for both \n# simple random sampling and systematic sampling is reported.\n\nsize_pop <- 1000\nsize_samp <- 100\n\n# defining the population randomly\npop <- runif(size_pop, min = 0, max = 1)\n\n# Simple random sampling\nsamp_cont <- sample(length(pop),size_samp)\nsamp <- pop[samp_cont]\n\nplot(density(pop), xlim=c(0,1), ylim=c(0,2), col=\"blue\", main=\"\", xlab=\"\")\npar(new=T)\nplot(density(samp), xlim=c(0,1), ylim=c(0,2), col=\"red\", main=\"\", xlab=\"\")\n\n# Systematic sampling\nsamp_cont <- seq(from = 1, to = size_pop, by = 10)\nsamp <- pop[samp_cont]\n\nplot(density(pop), xlim=c(0,1), ylim=c(0,2), col=\"blue\", main=\"\", xlab=\"\")\npar(new=T)\nplot(density(samp), xlim=c(0,1), ylim=c(0,2), col=\"red\", main=\"\", xlab=\"\")\n\n\n\n\n\n\n\nMethods: ggplot2 function of R\n\n\n\nggplot2 is a powerful data visualization function in R that allows you to create a wide range of high-quality plots for exploring and visualizing your data. The function is part of the ggplot2 package, which allows to add multiple layers to a given plot to create complex visualizations.\nIt is possible to use ggplot2 to create a wide range of plots, including scatter plots, bar plots, line plots, histograms, density plots, and more. The function works by defining the data you want to visualize, mapping variables to aesthetic properties such as color, size, and shape, and adding geometric objects such as points, lines, and bars to your plot. You can also customize your plot further by adding labels, legends, titles, and themes. Overall, ggplot is a versatile and flexible function that can be used to explore and communicate data in a clear and compelling way.\n\n\n\n# In the following R code, practical application for both \n# simple random sampling and systematic sampling is reported \n# by using ggplot2 function for the visualization.\n\nlibrary(ggplot2)\n\nset.seed(1)\nsize_pop <- 1000\nsize_samp <- 100\n\n# defining the population randomly\npop <- runif(size_pop, min = 0, max = 1)\n\n# Simple random sampling\nsamp_cont <- sample(length(pop), size_samp)\nsamp <- pop[samp_cont]\n\n\ndf_sampling <- data.frame(\n  values = c(pop, samp),\n  type = c(rep(\"pop\", length(pop)), rep('samp', length(samp)))\n)\n\nggplot(df_sampling) + \n  geom_density(aes(values, color = type, fill = type), alpha = 0.1) + \n  scale_color_manual(values = c(\"steelblue4\", \"firebrick2\")) + \n  scale_fill_manual(values = c(\"steelblue4\", \"firebrick2\")) + \n  theme_classic()"
  },
  {
    "objectID": "desc_stats.html#measures-of-central-tendency",
    "href": "desc_stats.html#measures-of-central-tendency",
    "title": "2  The Basics of Descriptive Statistics",
    "section": "2.2 Measures of Central Tendency",
    "text": "2.2 Measures of Central Tendency\nIn this section, various measures of centrality will be discussed. Undoubtedly, one of the most valuable techniques for summarizing data is to determine the average of a given set of data, which is a measure that describes the centrality of the dataset, and is commonly referred to as central tendency. There exist three distinct methods for describing the central location of a numerical dataset, namely the mean, median, and mode.\n\nThe mean: is the sum of all numbers belonging to the data set, divided by the number of all elements. More formally, we can define the mean as follows:\n\\[\n\\bar{x} = \\frac{\\sum_{i=1}^n x_i}{n}\n\\]\nwhere \\(n\\) is the number of the items belonging to the data set and \\(x_{i}\\) is the i-th item of the data set.\nWe need to make an important clarification. With the letter \\(\\bar{x}\\) we indicate the sample mean, given that \\(n\\) is the number of items selected for defining the sample. If instead we refer to the population mean, we should use the letter \\(\\mu\\). In this case the number of elements of the population are typically indicated with \\(N\\).\nThe median: is the middle item in a data set arranged in ascending/descending order.\nThe mode: is the highest occurring observation."
  },
  {
    "objectID": "desc_stats.html#measures-of-dispersion",
    "href": "desc_stats.html#measures-of-dispersion",
    "title": "2  The Basics of Descriptive Statistics",
    "section": "2.3 Measures of Dispersion",
    "text": "2.3 Measures of Dispersion\nThe study of measures of dispersion is of paramount importance in statistics as they provide crucial information regarding the extent of variability or spread within a given dataset. While measures of central tendency offer insight into the typical values of the dataset, measures of dispersion such as range, variance, and standard deviation provide information concerning the deviation of the data from the central tendency. A comprehensive understanding of the dispersion of the data is critical as it allows for more robust interpretation of statistical analysis results, facilitates identification of potential outliers or influential data points, and enables accurate assessment of the validity of statistical conclusions. In essence, measures of dispersion enable a more nuanced and comprehensive comprehension of the underlying phenomena. As shown below, we consider different definition of dispersion.\n\n2.3.1 The range\nRange is probably the simplest measure of dispersion in a data set. This is trivially calculated as the difference between the maximum value and the minimum value. It is a simple measure of spread that can provide a quick overview of the extent of variability within a dataset.\nFor example, suppose you have a dataset consisting of the following numbers: 4, 6, 7, 8, 9, 11. The maximum value in this dataset is 11, and the minimum value is 4. Therefore, the range of this dataset is calculated as follows:\n\\[\nrange = value_{max} - value_{min} = 11 - 4 = 7\n\\]\nSo, the range of this dataset is 7. This means that the values in the dataset are spread out over a range of 7 units.\nA more biological example may be the calculation of all contacts of a giver protein. Indeed, the range of interactions between atoms in a protein is highly dependent on the specific composition and structure of the protein, and the range of interaction distances can vary greatly between different proteins. Thus, by computing the range of interaction distances for a given protein, one can gain a more complete understanding of its unique structural and functional characteristics.\n\n\n2.3.2 The standard deviation\nThe standard deviation is a widely utilized measure of dispersion in data analysis, and it provides a quantification of how far data points deviate from the mean. In the context of this lecture series, which focuses on biological examples, we will examine the structural properties of proteins with particular emphasis. For illustrative purposes, we will focus on the myoglobin protein as an example.\n\n\n\n\n\n\nBiological focus: Myoglobin\n\n\n\nMyoglobin is an iron- and oxygen-binding protein present in the cardiac and skeletal muscle tissue. It is should be remembered that myoglobin was the first protein whose structure was experimentally solved with the x-ray technique. In order to analyze the three-dimensional structure of this protein, we consider the pdb code: 1MBN. The structure is made up of 153 residues, for a total of 1216 atoms.\n\n\nAs also shown in the R script below, we calculate the mean and standard deviation of the following set of numbers: the number of times each of the 20 amino acids appears in myoglobin sequence.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nALA\nARG\nASN\nASP\nGLN\nGLU\nGLY\nHIS\nILE\nLEU\nLYS\nMET\nPHE\nPRO\nSER\nTHR\nTRP\nTYR\nVAL\n\n\n\n\n17\n4\n1\n7\n5\n14\n11\n12\n9\n18\n19\n2\n6\n4\n6\n5\n2\n3\n8\n\n\n\nNotably, the total sum of frequencies for each amino acid occurrence in myoglobin is 153, corresponding precisely to the total number of residues comprising the protein.\nThe average number of times each amino acid appears in the protein sequence is: \\(\\bar{x} = 8.05\\).\nIn order to determine the deviation of each value in a given dataset from the mean, one can calculate the difference between the mean and each individual value. This computation is shown in the second row of the table presented below.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nALA\nARG\nASN\nASP\nGLN\nGLU\nGLY\nHIS\nILE\nLEU\nLYS\nMET\nPHE\nPRO\nSER\nTHR\nTRP\nTYR\nVAL\n\n\n\n\n17\n4\n1\n7\n5\n14\n11\n12\n9\n18\n19\n2\n6\n4\n6\n5\n2\n3\n8\n\n\n8.95\n-4.05\n-7.05\n-1.05\n-3.05\n5.95\n2.95\n3.95\n0.95\n9.95\n10.95\n-6.05\n-2.05\n-4.05\n-2.05\n-3.05\n-6.05\n-5.05\n-0.05\n\n\n\nWhen an amino acid appears more frequently than the average of the other amino acids, it is said to have positive dispersion; if it appears less frequently, it has negative dispersion. However, the previous definition of dispersion is not a reliable indicator, as the sum of all dispersion terms is zero by definition. To address this issue, we can calculate the squared value of each dispersion term, as presented in the third row of the table below.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nALA\nARG\nASN\nASP\nGLN\nGLU\nGLY\nHIS\nILE\nLEU\nLYS\nMET\nPHE\nPRO\nSER\nTHR\nTRP\nTYR\nVAL\n\n\n\n\n17\n4\n1\n7\n5\n14\n11\n12\n9\n18\n19\n2\n6\n4\n6\n5\n2\n3\n8\n\n\n8.95\n-4.05\n-7.05\n-1.05\n-3.05\n5.95\n2.95\n3.95\n0.95\n9.95\n10.95\n-6.05\n-2.05\n-4.05\n-2.05\n-3.05\n-6.05\n-5.05\n-0.05\n\n\n80.06\n16.42\n49.74\n1.11\n9.32\n35.37\n8.69\n15.58\n0.90\n98.95\n119.84\n36.63\n4.21\n16.42\n4.21\n9.32\n36.63\n25.53\n0.00\n\n\n\nWe define variance as the mean of these square deviations. More formally, we define the variance as follows:\n\\[\n\\sigma^2 = \\frac{\\sum_{i=1}^n (x_i - \\bar{x})^2}{n}\n\\]\nIn the case of our example, the variance is:\n\\[\n\\sigma^2 = \\frac{80.06 + 16.42 + 49.74 + … + 36.63 + 25.53 + 0.00}{19} = 29.94\n\\]\nThe disadvantage of the variance descriptor is that we will have the unit of measure squared, while the mean has the same unit of measure of the individual components of the data set. Therefore, an advantageous possibility is to define the standard deviation:\n\\[\n\\sigma = \\sqrt{variance} = \\sqrt{29.94} = 5.38\n\\]\n\n# In the following R code, mean, variance and standard deviation \n# for amino acid occurrence of the myoglobin protein are calculated.\n\nlibrary(bio3d)\n\n# importing pdb \npdb_aus <- read.pdb(\"1MBN\")\nfreq_aa <- table(pdb_aus$seqres)\n\n# sequence length control\nlength_control <- sum(freq_aa)\nprint(length_control)\n\n# mean calculation\ns_aus <- 0\nfor(i in 1:length(freq_aa)){\n  s_aus <- s_aus + freq_aa[i]\n}\nm_value_1 <- s_aus/length(freq_aa)\n\n# using R function for mean calculation\nm_value_2 <- mean(freq_aa)\n\n# dispersion\ndispersion <- freq_aa - m_value_2\nprint(dispersion)\n\n# square of the deviations\ndispersion_sq <- dispersion**2\nvariance_1 <- sum(dispersion_sq)/length(dispersion_sq)\nvariance_2 <- var(freq_aa)\n\n# standard deviation\nsdev_final <- sd(freq_aa)\n\n\n\n\n\n\n\nMethods: bio3d R package\n\n\n\nBio3D is a powerful R package designed for the analysis and visualization of biomolecular structures and sequence data. It provides a comprehensive suite of functions for the analysis and manipulation of protein and nucleic acid structures, including tools for protein structure alignment, superimposition, and analysis of structural dynamics. Additionally, Bio3D offers a range of functions for the analysis of protein and nucleic acid sequences, including tools for sequence alignment, phylogenetic analysis, and sequence motif discovery. Bio3D also provides a range of tools for the visualization of biomolecular structures and data, including functions for creating high-quality publication-ready plots and 3D visualizations. Overall, Bio3D is a versatile and powerful tool for the analysis and visualization of biomolecular data, and is widely used in the fields of structural biology, bioinformatics, and biophysics.\n\n\n\n\n2.3.3 The interquartile range\nThe interquartile range is a statistical measure of dispersion commonly used in data analysis when the median is used as the measure of central tendency. To fully grasp the concept of the interquartile range coefficient, it is important to first understand the quartile concept. Quartiles are widely used in statistical science to divide observations into four defined intervals. To determine quartiles, the data must be ordered either in ascending or descending order, in accordance with the definition of the median as the middle value of an ordered set of numbers. This enables the median to divide the data set into two equal parts, where half of the data lies below and above the median.\nThe quartiles breaks down the data, once sorted, into 4 groups so that \\(25 \\%\\) of the data are less than the lower quartile (\\(Q_{1}\\)), \\(50 \\%\\) are less than the median (\\(Q_{1}\\) and \\(Q_{2}\\)), and \\(75 \\%\\) are less than the last value (\\(Q_{1}\\), \\(Q_{2}\\) and \\(Q_{3}\\)). All elements of the data set greater than the last value, belong to the fourth quartile, which is made up of \\(25 \\%\\) of the data with the highest values (remember that the data is sorted).\nIn other words, quartiles in statistics are values that divide a set of data into four equal parts, with each part containing an equal number of data points. There are three quartiles: \\(Q1\\), \\(Q2\\) (also known as the median), and \\(Q3\\). \\(Q2\\) is the middle value of the data set, separating the lower half from the upper half. \\(Q1\\) is the value separating the lowest \\(25\\%\\) of the data from the remaining \\(75\\%\\), and \\(Q3\\) is the value separating the lowest \\(75\\%\\) of the data from the remaining \\(25\\%\\).\nNote that to divide the dataset into 4 parts we need 3 threshold values.\nThe interquartile range (IQR), which is the measure of dispersion discussed in these notes, encompasses both the second and third quartiles, thereby representing the central \\(50\\%\\) of the dataset. More specifically, the interquartile range of a set of variables is calculated as the difference between the upper and lower quartiles (the values between \\(Q_{1}\\) and \\(Q_{2}\\)). It is a measure of the spread in value of the central part of the data.\nQuartiles can be valuable measures of both central tendency and dispersion, offering valuable insights into the shape and range of a distribution.\n\\[\nIQR = UpperQuartile - LowerQuartile\n\\]\nAlso, if you want to get the quartile deviation coefficient (QDC), you should divide IQR index by the sum of the two quartiles.\n\\[\nQDC = \\frac{UpperQuartile - LowerQuartile}{UpperQuartile + LowerQuartile} = \\frac{Q_{3} - Q_{1}}{Q_{3} + Q_{1}}\n\\]\nTo determine quartiles for a dataset, we need to consider whether the number of elements is odd or even. When the number of data points is odd, the median is the central value of the sorted data, and we can choose whether to include (using the inclusive method) or exclude (using the exclusive method) the median. In contrast, when the number of data points is even, we must use the mean of the two middle values as the median.\nTo further illustrate the concept of quartiles, we will apply these theoretical principles to another example of biochemical interest.\n\n\n\n\n\n\nBiological focus: FMRP (Fragile X Mental Retardation Protein)\n\n\n\nFMRP is a widely expressed RNA-binding protein which play a crucial role for proper synaptic plasticity and architecture, which is critical for normal neural development and function.\nIt is encoded by the FMR1 human gene (FMRP translational regulator 1) and expressed in the brain and other tissues. Mutations or loss of FMRP function can lead to Fragile X syndrome, which is a genetic disorder that is the leading cause of inherited intellectual disability and autism spectrum disorders. FMRP is involved in the regulation of protein synthesis at synapses, and is thought to play a key role in the formation and maintenance of neural circuits in the brain.\n\n\nIn this particular biological example, we aim to analyze the distribution of B-factor values for each residue in the N-terminal domain of the FMRP protein. The B-factor, also known as the temperature factor, is a measure of the mobility of atoms within a protein crystal. It reflects the root-mean-square amplitude of atomic oscillation around their equilibrium positions, and is reported in the PDB file of a protein.\nIn essence, the B-factor indicates the level of atomic fluctuations in a crystal structure, even at low temperatures. Highly mobile regions of a protein tend to have higher B-factors, whereas more stable regions have lower B-factors.\nThe following R script calculates the interquartile range (IQR) as a measure of dispersion for the distribution of B-factor values of the c-alpha atoms.\n\n# In the following R code the interquatile range (IQR) \n# for beta factor values of FMRP N-terminal domain is calculated.\n\nlibrary(bio3d)\npdb_aus <- read.pdb(\"4qvz\")\ndf_coord <- pdb_aus$atom\n\n# Selecting only c-alpha atoms\ndf_coord_ca <- df_coord[df_coord$elety==\"CA\",]\n\n# Number of c-aplha (or number of residues)\nnrow(df_coord_ca)\n\n# Beta factor\nb_order <- sort(df_coord_ca$b)\neven_odd_control <- length(b_order) %% 2\n\n# even-numbered data set condition\nif(even_odd_control == 0){\n  median <- mean(b_order[(length(b_order)/2):((length(b_order)/2)+1)])\n  q1 <- b_order[round((length(b_order)/2)/2)]\n  q3 <- b_order[((length(b_order)/2)+1) + q1]\n  \n  # interquartile range\n  IQR <- q3-q1\n  print(IQR)\n  \n  # standard deviation\n  sdev <- sd(b_order)\n  print(sdev)\n}\n\n\n\n\n\n\n\nTest\n\n\n\nComparison of active state (pdb code: 1FMK) and inactive state (pdb code: 2SRC) of the c-Src kinase. To this end:\n\nCalculate for both structures the mean and the standard deviation of all distances among charged residues (CRs). Likewise calculate the mean and the standard deviation of all distances among uncharged residues (URs).\nCompare the CR-CR distance matrix (called \\(Mat_{CR}\\)) with UR-UR distance matrix (called \\(Mat_{UR}\\)) by computing a new matrix defined as the distance in percent:\n\\[\nPercDiffMat = \\frac{(Mat_{UR} - Mat_{CR})}{Mat_{CR}}\n\\]\nDiscuss the results obtained.\n\nAdditional Information\n\npositively charged amino acids: Lys, Arg and His.\nnegatively charged amino acids: Asp and Glu.\npolar amino acids: Ser, Thr, Tyr, Asn and Gln.\nnon-polar amino acids: Gly, Ala, Val, Cys, Pro, Leu, Ile, Met, Trp and Phe."
  },
  {
    "objectID": "stat_dist.html#absolute-and-relative-frequencies",
    "href": "stat_dist.html#absolute-and-relative-frequencies",
    "title": "3  Statistical Data Distributions",
    "section": "3.1 Absolute and relative frequencies",
    "text": "3.1 Absolute and relative frequencies\nIn statistics, the frequency of a given event is the number of times the specific event occurred. In this first part of the section, we will distinguish between relative frequency and absolute frequency.\nAbsolute frequency\nThe absolute frequency in simply the number of times that a value appears. Therefore, the sum of all absolute frequencies is the total number of data.\n\\[\nf_{1} + f_{2} + f_{3} + … + f_{n} = N\n\\]\nIn a more compact way:\n\\[\n\\sum_{i=1}^n f_i = N\n\\]\nRelative frequency\nThe relative frequency is obtained by dividing the absolute frequencies by the total number of data. Therefore, the sum of all relative frequencies is equal to 1. More formally:\n\\[\nn_i = \\frac{f_i}{N}\n\\]\n\n\n\n\n\n\nBiological focus: Monoclonal antibody\n\n\n\nThe monoclonal antibodies are a class of proteins designed in the laboratory in order to be able to bind specific targets. There are many types of monoclonal antibodies, but their common feature is that they can bind only one antigen. Monoclonal antibodies are often used in the diagnosis and treatment of many diseases, including some types of cancer.\nAs an example, we focus our attention on the monoclonal antibody for the influenza virus. More specifically, the crystal structure of the complex between neuraminidase from influenza virus (subtype N9 and isolated from an avian source) and the antigen-binding fragment (Fab) of monoclonal antibody NC41 has been refined (pdb code: 1NCA).\n\n\nTo better elucidate both the absolute and relative frequency concept, we analyze a specific biological problem. Here, the analysis of the contacts is performed for the case of a specific monoclonal antibody, which has been designed for the neuraminidase from influenza virus (pdb code: 1NCA). To this end, we define two residues in contact if the distance between their c-alpha atoms have a distance less than \\(12 Å\\).\nFirst of all, it is useful to describe the organization of the pdb file, which contains the information relating to the three-dimensional coordinates (x, y, z) of each atom of the protein.\nBelow, we report the first rows of the 1nca pdb file corresponding to the first c-alpha atoms of the protein.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntype\neleno\nelety\nalt\nresid\nchain\nresno\ninsert\nx\ny\nz\no\nb\nsegid\nelesy\ncharge\n\n\n\n\nATOM\n1\nN\nNA\nILE\nN\n81\nNA\n72.652\n8.334\n153.945\n1\n50.23\nNA\nN\nNA\n\n\nATOM\n2\nCA\nNA\nILE\nN\n81\nNA\n72.021\n8.573\n155.230\n1\n47.94\nNA\nC\nNA\n\n\nATOM\n3\nC\nNA\nILE\nN\n81\nNA\n71.384\n7.213\n155.534\n1\n45.77\nNA\nC\nNA\n\n\nATOM\n4\nO\nNA\nILE\nN\n81\nNA\n71.369\n6.388\n154.609\n1\n46.15\nNA\nO\nNA\n\n\nATOM\n5\nCB\nNA\nILE\nN\n81\nNA\n71.017\n9.779\n155.073\n1\n48.24\nNA\nC\nNA\n\n\nATOM\n6\nCG1\nNA\nILE\nN\n81\nNA\n70.453\n10.145\n156.446\n1\n49.92\nNA\nC\nNA\n\n\n\n\n\n\n\nFor each atom we have a set of information:\n\ntype of atom\nnumber of each element\natom name. Each atom in the structure of each residue is named with a specific nomenclature.\namino acid name (for example Ile, Arg, Asp, etc.)\nthe chain of belonging of each atom\nthe residue number\nthe coordinates of each atom (x, y and z)\nB-factor\ntype of atom, for example: C, N, O etc.\n\n\n\n\n\n\nFigure 3.1: Using the image function of R, we report the contact matrix of the antibody present in the 1nca pdb file. Each pair of residue are in contact if the distance between their c-alpha atoms have a distance lower then a certain threshold.\n\n\n\n\nIn Figure 3.1 the residue-residue contact matrix for the antibody of the 1nca pdb is reported.\nIn order to visualize the number of contacts of each residue, we report a plot where the residue number and the number of contacts are reported in the x-axis and y-axis, respectively. Interestingly, by analyzing the Figure Figure 3.2 is possible to note regions of the protein with a high number of contacts and regions with low number of contacts.\n\n\n\n\n\nFigure 3.2: For each residue the number of contacts is shown. Each pair of residue are in contact if the distance between their c-alpha atoms have a distance lower then \\(12 Å\\).\n\n\n\n\nIn fact, this analysis allow us to analyze some structural properties of the protein fold: regions with a higher number of contacts are located in the core of the protein, while regions with fewer contacts are populated by residues exposed to the solvent.\nThe purpose of this analysis is to analyze the absolute and relative frequencies of the number of contacts that each residue has with all its closest residues.\nTherefore, we calculate the number of residues that gave \\(n\\) contacts, where \\(n\\) ranges from 0 to the maximum number of contacts that a residue can make in this specific protein. We increase the value of \\(n\\) by adding 1 contacts to each step of the procedure. In the following table we report the absolute frequencies, i.e. how many residues have 0 contacts, how many residues have 1 contact, how many residues have 2 contacts, and so on.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n10\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n\n\n\n\n1\n2\n2\n3\n6\n4\n5\n9\n6\n17\n14\n14\n16\n19\n18\n10\n14\n16\n17\n14\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n\n\n\n\n12\n16\n16\n21\n21\n11\n19\n12\n17\n15\n11\n4\n10\n7\n4\n10\n3\n4\n5\n4\n4\n1\n1\n\n\n\n\n\n\n\nFor both tables the second row is “how many residues” and the first row is “have n contacts”. For example, we observe that there 19 residue interacting with 8 residues, as well as, there is 1 residue interacting with 44 residues. As described above, we calculate the relative frequency by dividing each element of absolute frequency by the total number of contacts.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n10\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n\n\n\n\n0.0023\n0.0046\n0.0046\n0.0069\n0.0138\n0.0092\n0.0115\n0.0207\n0.0138\n0.0391\n0.0322\n0.0322\n0.0368\n0.0437\n0.0414\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n\n\n\n\n0.023\n0.0322\n0.0368\n0.0391\n0.0322\n0.0276\n0.0368\n0.0368\n0.0483\n0.0483\n0.0253\n0.0437\n0.0276\n0.0391\n0.0345\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n\n\n\n\n0.0253\n0.0092\n0.023\n0.0161\n0.0092\n0.023\n0.0069\n0.0092\n0.0115\n0.0092\n0.0092\n0.0023\n0.0023\n\n\n\n\n\n\n\n\n# In the following R code, both absolute and relative frequencies \n# of residue contacts are calculated\n\nlibrary(bio3d)\npdb_aus <- read.pdb(\"1nca\")\ndf_coord <- pdb_aus$atom\n\n# setting parameters\ncutoff <- 12\n\n# atibody atom selection\ndf_coord_Ab <- df_coord[df_coord$chain == \"H\" | df_coord$chain == \"L\",]\n\n# Selecting only c-alpha atoms\ndf_coord_ca <- df_coord_Ab[df_coord_Ab$elety==\"CA\",]\ndf_coord_ca_xyz <- df_coord_ca[,c(\"x\",\"y\",\"z\")]\nhead(df_coord_ca)\n\n# contacts matrix definition\nDistMat <- as.matrix(dist(df_coord_ca_xyz))\nDistMat_Bin <- DistMat\nDistMat_Bin[DistMat_Bin <= cutoff] <- 1\nDistMat_Bin[DistMat_Bin > cutoff] <- 0\nimage(DistMat_Bin)\n\n# using the matrix for residue-residue pairs analysis.\n# in this case we need to remove the redundant information \n# due to symmetry properties of the distance matrix. \nDistMat_Bin_NoSym <- DistMat_Bin\nDistMat_Bin_NoSym[lower.tri(DistMat_Bin_NoSym, diag = FALSE)] <- 0\ndiag(DistMat_Bin_NoSym) <- 0\nimage(DistMat_Bin_NoSym)\n\n# On the contrary, for analysis conctacts of each node:\n# conctacts vector\ncontacts <- colSums(DistMat_Bin)\n\n# plot\nplot(\n  contacts, pch=20, cex=1.2, \n  xlab=\"Residue number\", ylab=\"Number of contacts\", \n  cex.lab=1.4, cex.axis=1.4, col=\"black\"\n)\nlines(contacts, col=\"blue\", lwd=1.4)\n\n# absolute frequencies\nabsF <- table(contacts)\n\n# relative frequencies\nrelF <- round(table(contacts)/sum(table(contacts)),4)\nsum(relF)\n\n\n# How to use ggplot \nlibrary(ggplot2)\n\ndf_ggplot <- data.frame(\n  ResNum = 1:length(contacts),\n  cont = contacts\n)\n\nggplot(df_ggplot, aes(x = ResNum, y = cont)) + \n  geom_line() + \n  theme_light() +\n  scale_color_manual(values = c(\"steelblue\")) +\n  labs(x = \"Residue Number\", y = \"Contacts\")"
  },
  {
    "objectID": "stat_dist.html#histograms",
    "href": "stat_dist.html#histograms",
    "title": "3  Statistical Data Distributions",
    "section": "3.2 Histograms",
    "text": "3.2 Histograms\nIn statistics, data is often represented using a histogram, which is constructed by dividing the data into a number of different intervals for each of which the number of the items is reported. Therefore, for each interval (or class) of data we associate the relative (or absolute) frequency, which corresponds to the area of the specific rectangle.\nCompared to the mean and variation calculation, the histogram provides a lot of additional information about the nature of the data. Indeed, by analyzing the shape of the distribution we can better investigate the properties of the data.\nTo construct a histogram from a continuous variable we need to split the data into intervals, called bins, and count the number of times that an event occur for each bin.\nIn order to understand the histogram definition also through an concrete application, we consider the following problem: To better describe the structural properties of a protein, may be useful to analyze the distribution of distances between a residue and any other. Therefore, according to the previous R script, the histogram of the distances can be obtained using hist function of R. In particular, as shown in Figure 3.3, each histogram can be obtained with different resolution. Indeed, changing the size of the bins we have a different count of items belonging to the given bin, meaning that the shape of the histogram can change according to the chosen bin size. The idea is that the two extreme situations (bins too large and bins too small) do not lead to useful information. In fact, if the size of the bins tends to zero, most of the bins are “empty” (no associated items). On the contrary, when the size of the bins tends to the maximum variation of the data set, all items belong to the single bin of the histogram. Both conditions are not informative. Therefore, the ideal condition is one that lives in the middle, allowing us to observe the shape of the histogram (which suggest us how the data is distributed). In Figure 3.3 are reported three cases that give us information about data distribution. In particular, for the three cases the breaks argument of hist function of R has changed. Defining the number of breaks, in fact, is one of the most important precautions in this type of analysis. Using the breaks argument it is possible to specify the number of cells we want in the histogram. Note that this number is only indicative for the R algorithm (it is only a suggestion), since R calculates the best number of bins for histogram definition, keeping however the information about the number of bins provided by the user.\n\n\n\n\n\nFigure 3.3: Residue-residue distance considering the c-alpha atoms of the 1nca pdb. Histogram with breaks = 100 (left), with breaks = 50 (center) and with breaks = 30 (right).\n\n\n\n\nIn the following R script we report a typical approach for histogram calculation.\n\n# In the following R code, the histrogram procedure is shown\n\nlibrary(bio3d)\npdb_aus <- read.pdb(\"1nca\")\ndf_coord <- pdb_aus$atom\n\n# setting parameters\ncutoff <- 12\n\n# atibody atom selection\ndf_coord_Ab <- df_coord[df_coord$chain == \"H\" | df_coord$chain == \"L\",]\n\n# Selecting only c-alpha atoms\ndf_coord_ca <- df_coord_Ab[df_coord_Ab$elety==\"CA\",]\ndf_coord_ca_xyz <- df_coord_ca[,c(\"x\",\"y\",\"z\")]\nhead(df_coord_ca)\n\n# contacts matrix definition\nDistMat <- as.matrix(dist(df_coord_ca_xyz))\n\npar(mfrow=c(1,3))\nhist(DistMat, col=\"blue\", ylim = c(0,25000), xlim=c(0,80), breaks=100, main=\"Histogram with breaks = 100\", \ncex.lab=1.4, cex.axis=1.6, cex.main=2, xlab=\"Distances\")\nhist(DistMat, col=\"blue\", ylim = c(0,25000), xlim=c(0,80), breaks=50, main=\"Histogram with breaks = 50\", \ncex.lab=1.4, cex.axis=1.6, cex.main=2, xlab=\"Distances\")\nhist(DistMat, col=\"blue\", ylim = c(0,25000), xlim=c(0,80), breaks=20, main=\"Histogram with breaks = 30\", \ncex.lab=1.4, cex.axis=1.6, cex.main=2, xlab=\"Distances\")\n\nDistMat_vet <- as.vector(DistMat)\nDistMat_vet_red <- sample(DistMat_vet,1000)\nh <- hist(DistMat_vet_red,ylim = c(0,50), xlim=c(0,80), breaks=100)\ntext(h$mids,h$counts,labels=h$counts, adj=c(0.5, -0.5))\n\n\n3.2.1 Interpretation of the histogram\nOne of the main purpose of these notes is to provide knowledge that allow us to interpret the nature of data through specific analysis. As mentioned before, the shape of an histogram is an important element for discuss about data distribution. For example, looking at Figure 3.3, we note that the shape of the distribution is not symmetric. The right tail of the histogram is more elongated than the left tail. Is there a reason for this asymmetry? Obviously yes. First of all, it is not possible to have a more elongated tail in the left part of the distribution because it is not possible to have distance between two atoms less then zero (since distance is a positive definite quantity). Therefore, there are more long-distance interactions than short-distance interactions. In general, the pairs of atoms (o residues) with the most frequent distance values are in the central part of the histogram, between \\(25 Å\\) and \\(35 Å\\). The most important effect of the data asimmetry is the following: the mean does not correspond with the mode (i.e. the value with the highest frequency), since the tail on the right helps to shift the mean in such direction.\n\n\n\n\n\n\nTest\n\n\n\nAnalysis of residue-residue interactions at the interface. In particular, taking into account a antibody-antigen complex (such as the pdb code: 3HFM), we would calculate:\n\nAn histogram of all \\(C_{\\alpha}-C_{\\alpha}\\) distances, just considering the intermolecular interactions. For this analysis we consider a contact between two residues only if the distance between their \\(C_{\\alpha}\\) atoms is less than \\(12 Å\\).\nCalculate the histogram of all interatomic distances increasing step by step the average hydrophobicity according to the Kyte Doolittle hydrophobicity scale."
  },
  {
    "objectID": "intr_prob.html#venn-diagram",
    "href": "intr_prob.html#venn-diagram",
    "title": "4  Introduction to probability",
    "section": "4.1 Venn Diagram",
    "text": "4.1 Venn Diagram\nIn probability, a Venn diagram is a way to describe logical relations between events. A typical Venn diagram is composed of several circles inside a rectangle. The circles represent the events, while the rectangle represents the sample space, which has previously been defined as the set of all possible outcomes. Each circle in the Venn diagram is within the sample space because represents an event, a subset of the sample space. In Figure 4.3 we consider a simple Venn diagram which is defined by two events, A and B.\n\n\n\n\n\nFigure 4.3: Venn Diagram for two events, A and B\n\n\n\n\nThe Venn diagram allows us to define not only the two events, A and B, but also to define the interactions between them. In particular, we can better visualize (and measure) the intersection space between two events (A \\(\\cap\\) B) and the union of the two event (A \\(\\cup\\) B)."
  },
  {
    "objectID": "intr_prob.html#probability-density-function",
    "href": "intr_prob.html#probability-density-function",
    "title": "4  Introduction to probability",
    "section": "4.2 Probability density function",
    "text": "4.2 Probability density function\nNow that the probability has been defined, we can introduce the Probability Density Function (PDF), as the continuous version of the histogram. Indeed, a PDF is obtained when the bin widths of the histogram are infinitesimal small. Similarly to the histogram, the probability density is distributed over the range of values describing our variable. By definition, the area under the PDF curve is equal to 1, which corresponds to the probability of having a value across the possible range of values. Therefore:\n\\[\n\\int_{-\\infty}^{+\\infty} \\rho(x) \\,dx = 1\n\\]\nMore interestingly, it is useful to calculate the probability that a value belongs to a given interval:\n\\[\nP(x \\in [x1,x2]) = \\int_{x_1}^{x_2} \\rho(x) \\,dx\n\\]\nin this case we calculate the probability that the variable \\(x\\) is between \\(x_1\\) and \\(x_2\\).\nHere, we take into account a specific biological example for discuss about the probability density. To this end, we select a system extensively studied in the last period: the complex composed of the spike protein of the SARS-CoV-2 coronavirus and the angiotensin-converting enzyme 2, or ACE2 receptor.\n\n\n\n\n\n\nBiological focus: The SARS-CoV-2 system\n\n\n\nSARS-CoV-2, similarly to SARS-CoV and MERS-CoV, attacks the lower respiratory system, thus causing viral pneumonia. It is well characterized that and how SARS-CoV infection is mediated by the high-affinity interactions between the receptor-binding domain (RBD) of the spike (S) glycoprotein and the human-host angiotensin-converting enzyme 2 (ACE2) receptor. The spike protein is located on the virus envelope and enables attachment to the host cell and the fusion between the virus and the cellular membrane. It has been shown that several critical residues in SARS-CoV-2’s RBD provide favorable interactions with human ACE2, consistent with SARS-CoV-2’s capability to infect the cell. From this point of view, the understanding of the molecular mechanism(s) of the interaction between the ACE2 receptor and the spike protein of the virus can be a key factor designing new drug compounds.\n\nMol Biosci. 2021 Jun 9;8:690655\n\n\n\nThe main goal of this analysis is to characterize the two interfaces of this specific molecular complex. In particular, we want to study the residue-residue distance distribution only considering the residues belonging to the two binding sites. To define the binding site we use the distance between the c-alpha atoms, considering two residues in contact if their c-alpha-c-alpha distance is less than \\(12 Å\\). To analyze the “compactness” of the two interface a local descriptor, which is a number associated to each interacting residue."
  },
  {
    "objectID": "r.html#sec-R",
    "href": "r.html#sec-R",
    "title": "Appendix A — Programming in R",
    "section": "A.1 Why R?",
    "text": "A.1 Why R?"
  },
  {
    "objectID": "r.html#sec-dataStructure",
    "href": "r.html#sec-dataStructure",
    "title": "Appendix A — Programming in R",
    "section": "A.2 Data structures",
    "text": "A.2 Data structures"
  },
  {
    "objectID": "r.html#sec-dataManipulation",
    "href": "r.html#sec-dataManipulation",
    "title": "Appendix A — Programming in R",
    "section": "A.3 Data manipulation",
    "text": "A.3 Data manipulation"
  },
  {
    "objectID": "r.html#sec-dataRepresentation",
    "href": "r.html#sec-dataRepresentation",
    "title": "Appendix A — Programming in R",
    "section": "A.4 Data representation",
    "text": "A.4 Data representation"
  }
]