# The Basics of Descriptive Statistics

Descriptive Statistics (DS) is a branch of statistics whose purpose is to collect, organize, visualize and analyze data taken from a population. DS is composed of a number of methods and definitions, in order to quantitatively treat data and interpret corresponding results in statistical terms. Furthermore, DS differs from inferential statistics in that it is not based on probability theory.

## Difference between population and sample

A population refers to the entire group of items from which one wants to draw conclusions, while a sample is a specific subset of the entire population from which data is collected. Therefore, the sample size is always smaller than the total population size. In practical terms, the use of a sample is particularly important when the population is large and it is not feasible to analyze every component of it. For instance, in order to determine the voting percentages of each political party, theoretically, each adult would need to be asked about their favorite party. However, this is a complex and almost impossible task, which is why companies that propose polls for voting percentage work with samples, such as exit polls.

The critical issue is to investigate the conditions under which the prediction is close to the reality, as well as to define what "close" means in this context. Using statistical analysis, it is possible to use sample data to obtain estimates or test hypotheses about population data. Ideally, when the sample size is similar to the population size, the results can be estimated reliably, since the sample is necessarily a good representation of the population. However, in most cases, the population is only a theoretical concept, and it is not feasible to work with it. Therefore, it is necessary to choose a criterion to appropriately select the working sample in such a way that it is as representative as possible of the population. The number of objects/subjects that should be included in a sample depends on multiple factors, such as the size and variability of the population or the purpose of the analysis.

There are two types of approaches for sampling population data to obtain a suitable sample: (i) probability sampling methods and (ii) non-probability sampling methods.

**Probability sampling methods** Probability sampling means that each element of the population can be selected. Typically this is the best method if you want to produce results that are representative of the whole population. There are four main types of probability sample, below all these will be briefly discuss.

-   <u>Simple random sampling</u> each element of the population has the same probability of being selected for the definition of the sample. Therefore, the whole population can be subject to random extraction. A useful tool, in computational terms, is the random number generator (see next example in code R).

-   <u>Systematic sampling</u> the basic idea is to imagine the population dataset as a list and select the elements of the sample with a specific regularity (considering fixed steps). Obviously, this method is very similar to the previous one if the dataset is uniform. On the contrary, if the population dataset has hidden patters, the sample risks being out of balance with the population.

-   <u>Stratified sampling</u> In this case, the data is stratified (subpopulation groups are created from the population) in such a way that each group is well represented in the population. At this point, for the construction of the sample, members can be randomly selected from each sample in order to maintain the proportion with the initial population.

-   <u>Cluster sampling</u> consists in dividing the population into a certain number of groups, which are obtained thanks to a clustering analysis (we will see this in the next chapters). Each subgroup should have similar characteristics to the entire sample, allowing us to randomly sample entire subgroups selections.

**Non-Probability sampling methods** In a non-probability sample, members belonging to the sample are selected based on non-random criteria, meaning that not every individual has a chance of being included. This type of sample is easier and cheaper to access, but it has a higher risk of sampling bias. Also in this case there are four different kind of non probability sampling approaches.

-   <u>A convenience sample</u> is simply made up of subjects/objects that are more accessible to the researcher. This is typically a first exploratory approach, which is simple and inexpensive, which can be useful for initial data collection. However, the most important limitation is that there is no way to state whether the sample is representative of the population and so, in principle, it cannot produce generalizable results.

-   <u>Voluntary response sampling</u> is a method that refers to all those data that derive from voluntary sharing. One common bias is self-selection bias, where individuals who choose to participate in the study may be different from those who choose not to participate, leading to a non-representative sample. Additionally, voluntary response sampling may attract individuals with extreme opinions or experiences, which can also lead to biased results.

-   <u>Purposive sampling</u> is a type of sampling, also known as judgment sampling, which involves the use of the researcher's experience, given that the sample is selected in a specific way considering a series of constraints useful for the purposes of the research. Purposive sampling is used in statistics where the researcher deliberately chooses individuals or units to be included in the sample based on a specific purpose or criterion. In this method, the researcher selects participants who are believed to be most appropriate or relevant for the research question, rather than selecting them randomly.

-   <u>Snowball sampling</u> is characteristic for samples made up of people. If the population is difficult to access, snowball sampling can be used to engage participants via other participants (via a first neighbors selection criterion).

```{r, eval=FALSE}
# In the following R code, practical application for both 
# simple random sampling and systematic sampling is reported.

size_pop <- 1000
size_samp <- 100

# defining the population randomly
pop <- runif(size_pop, min = 0, max = 1)

# Simple random sampling
samp_cont <- sample(length(pop),size_samp)
samp <- pop[samp_cont]

plot(density(pop), xlim=c(0,1), ylim=c(0,2), col="blue", main="", xlab="")
par(new=T)
plot(density(samp), xlim=c(0,1), ylim=c(0,2), col="red", main="", xlab="")

# Systematic sampling
samp_cont <- seq(from = 1, to = size_pop, by = 10)
samp <- pop[samp_cont]

plot(density(pop), xlim=c(0,1), ylim=c(0,2), col="blue", main="", xlab="")
par(new=T)
plot(density(samp), xlim=c(0,1), ylim=c(0,2), col="red", main="", xlab="")
```

::: callout-note
## Methods: `ggplot2` function of R

`ggplot2` is a powerful data visualization function in R that allows you to create a wide range of high-quality plots for exploring and visualizing your data. The function is part of the `ggplot2` package, which allows to add multiple layers to a given plot to create complex visualizations.

It is possible to use `ggplot2` to create a wide range of plots, including scatter plots, bar plots, line plots, histograms, density plots, and more. The function works by defining the data you want to visualize, mapping variables to aesthetic properties such as color, size, and shape, and adding geometric objects such as points, lines, and bars to your plot. You can also customize your plot further by adding labels, legends, titles, and themes. Overall, ggplot is a versatile and flexible function that can be used to explore and communicate data in a clear and compelling way.
:::

```{r, eval=FALSE}
# In the following R code, practical application for both 
# simple random sampling and systematic sampling is reported 
# by using ggplot2 function for the visualization.

library(ggplot2)

set.seed(1)
size_pop <- 1000
size_samp <- 100

# defining the population randomly
pop <- runif(size_pop, min = 0, max = 1)

# Simple random sampling
samp_cont <- sample(length(pop), size_samp)
samp <- pop[samp_cont]


df_sampling <- data.frame(
  values = c(pop, samp),
  type = c(rep("pop", length(pop)), rep('samp', length(samp)))
)

ggplot(df_sampling) + 
  geom_density(aes(values, color = type, fill = type), alpha = 0.1) + 
  scale_color_manual(values = c("steelblue4", "firebrick2")) + 
  scale_fill_manual(values = c("steelblue4", "firebrick2")) + 
  theme_classic()
```

## Measures of Central Tendency

In this section, various measures of centrality will be discussed. Undoubtedly, one of the most valuable techniques for summarizing data is to determine the average of a given set of data, which is a measure that describes the centrality of the dataset, and is commonly referred to as central tendency. There exist three distinct methods for describing the central location of a numerical dataset, namely the mean, median, and mode.

-   <u>The mean:</u> is the sum of all numbers belonging to the data set, divided by the number of all elements. More formally, we can define the mean as follows:

    $$
    \bar{x} = \frac{\sum_{i=1}^n x_i}{n}
    $$

    where $n$ is the number of the items belonging to the data set and $x_{i}$ is the i-th item of the data set.

    We need to make an important clarification. With the letter $\bar{x}$ we indicate the sample mean, given that $n$ is the number of items selected for defining the sample. If instead we refer to the population mean, we should use the letter $\mu$. In this case the number of elements of the population are typically indicated with $N$.

-   <u>The median:</u> is the middle item in a data set arranged in ascending/descending order.

-   <u>The mode:</u> is the highest occurring observation.

## Measures of Dispersion

The study of measures of dispersion is of paramount importance in statistics as they provide crucial information regarding the extent of variability or spread within a given dataset. While measures of central tendency offer insight into the typical values of the dataset, measures of dispersion such as range, variance, and standard deviation provide information concerning the deviation of the data from the central tendency. A comprehensive understanding of the dispersion of the data is critical as it allows for more robust interpretation of statistical analysis results, facilitates identification of potential outliers or influential data points, and enables accurate assessment of the validity of statistical conclusions. In essence, measures of dispersion enable a more nuanced and comprehensive comprehension of the underlying phenomena. As shown below, we consider different definition of dispersion.

### The range

Range is probably the simplest measure of dispersion in a data set. This is trivially calculated as the difference between the maximum value and the minimum value. It is a simple measure of spread that can provide a quick overview of the extent of variability within a dataset.

For example, suppose you have a dataset consisting of the following numbers: 4, 6, 7, 8, 9, 11. The maximum value in this dataset is 11, and the minimum value is 4. Therefore, the range of this dataset is calculated as follows:

$$
range = value_{max} - value_{min} = 11 - 4 = 7
$$

So, the range of this dataset is 7. This means that the values in the dataset are spread out over a range of 7 units.

A more biological example may be the calculation of all contacts of a giver protein. Indeed, the range of interactions between atoms in a protein is highly dependent on the specific composition and structure of the protein, and the range of interaction distances can vary greatly between different proteins. Thus, by computing the range of interaction distances for a given protein, one can gain a more complete understanding of its unique structural and functional characteristics.

### The standard deviation

The standard deviation is a widely utilized measure of dispersion in data analysis, and it provides a quantification of how far data points deviate from the mean. In the context of this lecture series, which focuses on biological examples, we will examine the structural properties of proteins with particular emphasis. For illustrative purposes, we will focus on the myoglobin protein as an example.

::: callout-tip
## Biological focus: Myoglobin

Myoglobin is an iron- and oxygen-binding protein present in the cardiac and skeletal muscle tissue. It is should be remembered that myoglobin was the first protein whose structure was experimentally solved with the x-ray technique. In order to analyze the three-dimensional structure of this protein, we consider the pdb code: 1MBN. The structure is made up of 153 residues, for a total of 1216 atoms.
:::

As also shown in the R script below, we calculate the mean and standard deviation of the following set of numbers: the number of times each of the 20 amino acids appears in myoglobin sequence.

```{=html}
<style scoped> table { font-size: 10px; text-align: center; } td, th {padding: 4px 0 !important; } col { width: 5% !important; }</style>
```
| ALA | ARG | ASN | ASP | GLN | GLU | GLY | HIS | ILE | LEU | LYS | MET | PHE | PRO | SER | THR | TRP | TYR | VAL |
|-----|-----|-----|-----|-----|-----|-----|-----|-----|-----|-----|-----|-----|-----|-----|-----|-----|-----|-----|
| 17  | 4   | 1   | 7   | 5   | 14  | 11  | 12  | 9   | 18  | 19  | 2   | 6   | 4   | 6   | 5   | 2   | 3   | 8   |

Notably, the total sum of frequencies for each amino acid occurrence in myoglobin is 153, corresponding precisely to the total number of residues comprising the protein.

The average number of times each amino acid appears in the protein sequence is: $\bar{x} = 8.05$.

In order to determine the deviation of each value in a given dataset from the mean, one can calculate the difference between the mean and each individual value. This computation is shown in the second row of the table presented below.

```{=html}
<style scoped> table { font-size: 10px; text-align: center; } td, th {padding: 4px 0 !important; } col { width: 5% !important; }</style>
```
| ALA  | ARG   | ASN   | ASP   | GLN   | GLU  | GLY  | HIS  | ILE  | LEU  | LYS   | MET   | PHE   | PRO   | SER   | THR   | TRP   | TYR   | VAL   |
|------|-------|-------|-------|-------|------|------|------|------|------|-------|-------|-------|-------|-------|-------|-------|-------|-------|
| 17   | 4     | 1     | 7     | 5     | 14   | 11   | 12   | 9    | 18   | 19    | 2     | 6     | 4     | 6     | 5     | 2     | 3     | 8     |
| 8.95 | -4.05 | -7.05 | -1.05 | -3.05 | 5.95 | 2.95 | 3.95 | 0.95 | 9.95 | 10.95 | -6.05 | -2.05 | -4.05 | -2.05 | -3.05 | -6.05 | -5.05 | -0.05 |

When an amino acid appears more frequently than the average of the other amino acids, it is said to have positive dispersion; if it appears less frequently, it has negative dispersion. However, the previous definition of dispersion is not a reliable indicator, as the sum of all dispersion terms is zero by definition. To address this issue, we can calculate the squared value of each dispersion term, as presented in the third row of the table below.

```{=html}
<style scoped> table { font-size: 10px; text-align: center; } td, th {padding: 4px 0 !important; } col { width: 5% !important; }</style>
```
| ALA   | ARG   | ASN   | ASP   | GLN   | GLU   | GLY  | HIS   | ILE  | LEU   | LYS    | MET   | PHE   | PRO   | SER   | THR   | TRP   | TYR   | VAL   |
|-------|-------|-------|-------|-------|-------|------|-------|------|-------|--------|-------|-------|-------|-------|-------|-------|-------|-------|
| 17    | 4     | 1     | 7     | 5     | 14    | 11   | 12    | 9    | 18    | 19     | 2     | 6     | 4     | 6     | 5     | 2     | 3     | 8     |
| 8.95  | -4.05 | -7.05 | -1.05 | -3.05 | 5.95  | 2.95 | 3.95  | 0.95 | 9.95  | 10.95  | -6.05 | -2.05 | -4.05 | -2.05 | -3.05 | -6.05 | -5.05 | -0.05 |
| 80.06 | 16.42 | 49.74 | 1.11  | 9.32  | 35.37 | 8.69 | 15.58 | 0.90 | 98.95 | 119.84 | 36.63 | 4.21  | 16.42 | 4.21  | 9.32  | 36.63 | 25.53 | 0.00  |

We define **variance** as the mean of these square deviations. More formally, we define the variance as follows:

$$
\sigma^2 = \frac{\sum_{i=1}^n (x_i - \bar{x})^2}{n}
$$

In the case of our example, the variance is:

$$
\sigma^2 = \frac{80.06 + 16.42 + 49.74 + … + 36.63 + 25.53 + 0.00}{19} = 29.94
$$

The disadvantage of the variance descriptor is that we will have the unit of measure squared, while the mean has the same unit of measure of the individual components of the data set. Therefore, an advantageous possibility is to define the standard deviation:

$$
\sigma = \sqrt{variance} = \sqrt{29.94} = 5.38
$$

```{r, eval=FALSE}
# In the following R code, mean, variance and standard deviation 
# for amino acid occurrence of the myoglobin protein are calculated.

library(bio3d)

# importing pdb 
pdb_aus <- read.pdb("1MBN")
freq_aa <- table(pdb_aus$seqres)

# sequence length control
length_control <- sum(freq_aa)
print(length_control)

# mean calculation
s_aus <- 0
for(i in 1:length(freq_aa)){
  s_aus <- s_aus + freq_aa[i]
}
m_value_1 <- s_aus/length(freq_aa)

# using R function for mean calculation
m_value_2 <- mean(freq_aa)

# dispersion
dispersion <- freq_aa - m_value_2
print(dispersion)

# square of the deviations
dispersion_sq <- dispersion**2
variance_1 <- sum(dispersion_sq)/length(dispersion_sq)
variance_2 <- var(freq_aa)

# standard deviation
sdev_final <- sd(freq_aa)
```

::: callout-note
## Methods: bio3d R package

Bio3D is a powerful R package designed for the analysis and visualization of biomolecular structures and sequence data. It provides a comprehensive suite of functions for the analysis and manipulation of protein and nucleic acid structures, including tools for protein structure alignment, superimposition, and analysis of structural dynamics. Additionally, Bio3D offers a range of functions for the analysis of protein and nucleic acid sequences, including tools for sequence alignment, phylogenetic analysis, and sequence motif discovery. Bio3D also provides a range of tools for the visualization of biomolecular structures and data, including functions for creating high-quality publication-ready plots and 3D visualizations. Overall, Bio3D is a versatile and powerful tool for the analysis and visualization of biomolecular data, and is widely used in the fields of structural biology, bioinformatics, and biophysics.
:::

### **The interquartile range**

The interquartile range is a statistical measure of dispersion commonly used in data analysis when the median is used as the measure of central tendency. To fully grasp the concept of the interquartile range coefficient, it is important to first understand the quartile concept. Quartiles are widely used in statistical science to divide observations into four defined intervals. To determine quartiles, the data must be ordered either in ascending or descending order, in accordance with the definition of the median as the middle value of an ordered set of numbers. This enables the median to divide the data set into two equal parts, where half of the data lies below and above the median.

The quartiles breaks down the data, once sorted, into 4 groups so that $25 \%$ of the data are less than the lower quartile ($Q_{1}$), $50 \%$ are less than the median ($Q_{1}$ and $Q_{2}$), and $75 \%$ are less than the last value ($Q_{1}$, $Q_{2}$ and $Q_{3}$). All elements of the data set greater than the last value, belong to the fourth quartile, which is made up of $25 \%$ of the data with the highest values (remember that the data is sorted).

In other words, quartiles in statistics are values that divide a set of data into four equal parts, with each part containing an equal number of data points. There are three quartiles: $Q1$, $Q2$ (also known as the median), and $Q3$. $Q2$ is the middle value of the data set, separating the lower half from the upper half. $Q1$ is the value separating the lowest $25\%$ of the data from the remaining $75\%$, and $Q3$ is the value separating the lowest $75\%$ of the data from the remaining $25\%$.

Note that to divide the dataset into 4 parts we need 3 threshold values.

The interquartile range (IQR), which is the measure of dispersion discussed in these notes, encompasses both the second and third quartiles, thereby representing the central $50\%$ of the dataset. More specifically, the interquartile range of a set of variables is calculated as the difference between the upper and lower quartiles (the values between $Q_{1}$ and $Q_{2}$). It is a measure of the spread in value of the central part of the data.

Quartiles can be valuable measures of both central tendency and dispersion, offering valuable insights into the shape and range of a distribution.

$$
IQR = UpperQuartile - LowerQuartile
$$

Also, if you want to get the quartile deviation coefficient (QDC), you should divide IQR index by the sum of the two quartiles.

$$
QDC = \frac{UpperQuartile - LowerQuartile}{UpperQuartile + LowerQuartile} = \frac{Q_{3} - Q_{1}}{Q_{3} + Q_{1}}
$$

To determine quartiles for a dataset, we need to consider whether the number of elements is odd or even. When the number of data points is odd, the median is the central value of the sorted data, and we can choose whether to include (using the inclusive method) or exclude (using the exclusive method) the median. In contrast, when the number of data points is even, we must use the mean of the two middle values as the median.

To further illustrate the concept of quartiles, we will apply these theoretical principles to another example of biochemical interest.

::: callout-tip
## Biological focus: FMRP (Fragile X Mental Retardation Protein)

FMRP is a widely expressed RNA-binding protein which play a crucial role for proper synaptic plasticity and architecture, which is critical for normal neural development and function.

It is encoded by the FMR1 human gene (FMRP translational regulator 1) and expressed in the brain and other tissues. Mutations or loss of FMRP function can lead to Fragile X syndrome, which is a genetic disorder that is the leading cause of inherited intellectual disability and autism spectrum disorders. FMRP is involved in the regulation of protein synthesis at synapses, and is thought to play a key role in the formation and maintenance of neural circuits in the brain.
:::

In this particular biological example, we aim to analyze the distribution of B-factor values for each residue in the N-terminal domain of the FMRP protein. The B-factor, also known as the temperature factor, is a measure of the mobility of atoms within a protein crystal. It reflects the root-mean-square amplitude of atomic oscillation around their equilibrium positions, and is reported in the PDB file of a protein.

In essence, the B-factor indicates the level of atomic fluctuations in a crystal structure, even at low temperatures. Highly mobile regions of a protein tend to have higher B-factors, whereas more stable regions have lower B-factors.

The following R script calculates the interquartile range (IQR) as a measure of dispersion for the distribution of B-factor values of the c-alpha atoms.

```{r eval=FALSE}
# In the following R code the interquatile range (IQR) 
# for beta factor values of FMRP N-terminal domain is calculated.

library(bio3d)
pdb_aus <- read.pdb("4qvz")
df_coord <- pdb_aus$atom

# Selecting only c-alpha atoms
df_coord_ca <- df_coord[df_coord$elety=="CA",]

# Number of c-aplha (or number of residues)
nrow(df_coord_ca)

# Beta factor
b_order <- sort(df_coord_ca$b)
even_odd_control <- length(b_order) %% 2

# even-numbered data set condition
if(even_odd_control == 0){
  median <- mean(b_order[(length(b_order)/2):((length(b_order)/2)+1)])
  q1 <- b_order[round((length(b_order)/2)/2)]
  q3 <- b_order[((length(b_order)/2)+1) + q1]
  
  # interquartile range
  IQR <- q3-q1
  print(IQR)
  
  # standard deviation
  sdev <- sd(b_order)
  print(sdev)
}
```

::: callout-caution
## Test

Comparison of active state (pdb code: 1FMK) and inactive state (pdb code: 2SRC) of the c-Src kinase. To this end:

1.  Calculate for both structures the mean and the standard deviation of all distances among charged residues (CRs). Likewise calculate the mean and the standard deviation of all distances among uncharged residues (URs).

2.  Compare the CR-CR distance matrix (called $Mat_{CR}$) with UR-UR distance matrix (called $Mat_{UR}$) by computing a new matrix defined as the distance in percent:

    $$
    PercDiffMat = \frac{(Mat_{UR} - Mat_{CR})}{Mat_{CR}}
    $$

3.  Discuss the results obtained.

<u>Additional Information</u>

-   positively charged amino acids: Lys, Arg and His.

-   negatively charged amino acids: Asp and Glu.

-   polar amino acids: Ser, Thr, Tyr, Asn and Gln.

-   non-polar amino acids: Gly, Ala, Val, Cys, Pro, Leu, Ile, Met, Trp and Phe.
:::
